Bascis:-
VM_enablement: enable VM_Component in BIOS when Booting for VM-Box to wrok.
Install VM_box extensions
VM_SHared Floder: General->Advanced->Biderectional->Shared Floder->Floder-> Auto mounth
                  sudo gpasswd -a <username> vboxsf
                  sudo reboot
                  home/media/Sf_floder
                  sudo ln -s /media/sf_Shared_VM_Floder /home/cloudera/Desktop/testsf (to create a softlink to Shared folder)
VM_Network(for connecting to putty and fillazilla): Adapter2-Host-onlyAdapter
            PromiscuousMode -Deny
for additioal information on trouble shooting please look at
http://www.oracle.com/technetwork/database/bigdata-appliance/bigdatalite-quickdeploy-460-3301260.pdf

Installing JAVA:
Install JDK
check JAVA_HOME, PATH and CLASSPATH variables in system_variables for previous versions.
Update bin's of JRE and JDK in PATH:: Update JAVA_HOME to JDK in C-Drive:: Update bin's of JRE and JDK in CLASSPATH with ;. at the end.
Check using echo %path%:: echo %JAVA_HOME%:: javac
            
CDH Versions:-(minimum version for spark script running in Python is Python 2.6)
hadoop version:: hive --version:: Sqoop version:: Spark-shell version:: sc.version:: Python -V:: java -version:: javac:: hive-site.xml(/etc/hive/conf/hive-site.xml)
Spark is written in Scala
Hive and Map/Reduce is written in Java
Impala is written in c++.
Python is a dynamic(cann't be complied to jar) and functional programing laguage.
Scala is a static yet dynamic-looking and a mix between functional and object-oriented language.
Java is an static and object-oriented programing languague.

Spark COmponents:-
spark-shell, pyspark
sparkcontext:: sqlcontext:: hivecontext:: spark-sql[avaible from 1.5.2]:: JDBC
to access HiveContext:
sudo ln -s /etc/hive/conf/hive-site.xml \
           /etc/spark/conf/hive-site.xml
spark-shell
sqlcontext.sql("select * from Table_name").collect().foreach(println)

Starting Cloudera Maneger:-
sudo /home/cloudera/cloudera-manager --express --force

Removing files with out prompting recursively:-
rm -rf /directory/path

IDE Set-up:-
To findout system version in Linux based systems use "uname -i" command.
check: CMD(javac:: JDK)
Download JDK from Oracle.
Path:- c:\Program_File\java\jdks\(add path by using';')
eclipse-> windows -> preferences -> Java -> installed JRES -> add -> standrad VM -> c:\ProgramFiles\java\jdk8
Maven setup: help->install new software->Check for Available Software->Copy m2e releases repository link->Browse for it in Available Softwares tab->Next

Running Python Shell from CMD:-
c:\Python34\python.exe c:\user\sg0952655\desktop\new.py arg1 arg2 arg3

Running Java application:
javac ExampleProgram.java
java ExampleProgram

Using Putty from local host:
For CDH:-
ifconfig -a (to get IP adress)
ssh cloudera@IP_adress (to connect to Hadoop cluster)
scp dectofcards.txt cloudera@192.168.90.131:/home/cloudera (to copy files to hadoop)
access Hue: IP_adress:8888 (192.168.90.131:8888)
access NameNode: IP_adress:50070 (192.168.90.131:50070)  
(OR)  
The CDH VM box also offer port forwarding on port 22  
The VM box can be routed for ssh operations on port 22 by going to Network->Adapter 1(NAT)->Port Forwarding  

For HDP:-
ifconfig -a (to get IP adress)
ssh root@127.0.0.1 2222(to connect to Hadoop cluster)
access HDP UI: 127.0.0.1:8888 (IP_adress:8888)
access Ambari: 127.0.0.1:8080 (IP_adress:8080)

To find the cloudera Repositories for dependencies:
find /opt -name "*hadoop*.jar"
(or for a VM based version)
find /usr -name "*hadoop*.jar"
find /usr -name "*hadoop-mapreduce_client-common*.jar"

To check the HortonWorks cluster for ambari and HDP version:
ambari: rpm -qa |grep ambari-server
        rpm -qa |grep ambari-agent
HDP: hdp-select | grep hadoop-client
     hdp-select versions
     hdp-select status

To Check the jar content:
jar tvf jar/location/jars.jar

To connect to mysql using Tools in Coludera_VM:
mysql -u retail_dba -P
password: cloudera
